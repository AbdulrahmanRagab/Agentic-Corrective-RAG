README.md
markdownDownloadCopy code# ğŸ¤– Agentic RAG Assistant (LangGraph + LangChain + Streamlit + FAISS)

A Streamlit app that implements an **agentic RAG** workflow: the LLM decides **when** to retrieve, **grades** retrieved context for relevance, **rewrites** the query if retrieval fails, and then **generates** an answer grounded in the retrieved sources.

---

## ğŸ§­ Table of Contents (Overview)

1.  ğŸ¤– [Overview](#overview)  
2.  âœ¨ [Features](#features)  
3.  ğŸ§° [Tech Stack](#tech-stack)  
4.  ğŸ§  [Architecture](#architecture)  
    -  ğŸ” [LangGraph Flow](#langgraph-flow)  
    -  ğŸ“š [Knowledge Base & Retrieval Tools](#knowledge-base--retrieval-tools)  
5.  ğŸš€ [Getting Started](#getting-started)  
    -  âœ… [Prerequisites](#prerequisites)  
    -  ğŸ“¦ [Installation](#installation)  
    -  â–¶ï¸ [Run the App](#run-the-app)  
6.  âš™ï¸ [Configuration (UI)](#configuration-ui)  
7.  ğŸ” [How It Works (Stepâ€‘byâ€‘step)](#how-it-works-step-by-step)  
8.  ğŸ› ï¸ [Customization](#customization)  
9.  ğŸ§¯ [Troubleshooting](#troubleshooting)  
10. âš ï¸ [Known Limitations](#known-limitations)  
11. ğŸ” [Security Notes](#security-notes)  
12. ğŸ—ºï¸ [Roadmap Ideas](#roadmap-ideas)  
13. ğŸ™ [Acknowledgements / Sources](#acknowledgements--sources)  
14. ğŸ“„ [License](#license)  
15. ğŸ“ [Project Structure](#project-structure)  

---

<a id="overview"></a>
## ğŸ¤– Overview

This project is a singleâ€‘page Streamlit application called **Agentic RAG Assistant**. It lets users ask questions about:

- ğŸ§© **AI Agents** (ReAct prompting, tool use, agent loops)  
- ğŸ“ **RAGAS** (RAG evaluation framework and metrics)  

Instead of always retrieving documents, the assistant behaves more like an **agent**:

- âœ… It can **answer directly** when retrieval isnâ€™t needed.  
- ğŸ”§ Or it can **invoke retrieval tools**, **check relevance**, and **retry** with a rewritten query if the retrieved content doesnâ€™t match the question.

---

<a id="features"></a>
## âœ¨ Features

- ğŸ§  **Multiâ€‘provider UI** (OpenAI / Groq + placeholders for OpenRouter / Gemini)  
- ğŸ›ï¸ **Model selection** from the sidebar  
- ğŸ”‘ **API key input** in the sidebar (password field)  
- ğŸ§¾ **Max response tokens** control  
- ğŸ§ª **Agent step tracing** (â€œShow Agent Stepsâ€) to inspect node execution and outputs  
- ğŸ§° **Two retriever tools** over two different webâ€‘based knowledge sets  
- âœ… **Relevance grading** step to detect bad retrieval results  
- â™»ï¸ **Query rewriting loop** when retrieval is not relevant  
- ğŸ—„ï¸ **FAISS vector stores** built onâ€‘theâ€‘fly and cached with `st.cache_resource`  
- ğŸ’¡ **Suggested question buttons** for quick testing  
- ğŸ§¹ **Clear chat** button  
- ğŸ” **Rebuild Pipeline** button to clear cache and rebuild the knowledge base  

---

<a id="tech-stack"></a>
## ğŸ§° Tech Stack

- ğŸ–¥ï¸ **UI**: Streamlit  
- ğŸ§­ **Agent Orchestration**: LangGraph  
- ğŸ¤ **LLM Integration**: LangChain (`ChatOpenAI`, `ChatGroq`)  
- ğŸ§  **Retrieval / Vector DB**: FAISS (inâ€‘memory)  
- ğŸ§¬ **Embeddings**: HuggingFace embeddings (local path if available, fallback otherwise)  
- ğŸŒ **Data Loading**: `WebBaseLoader` (loads web pages as documents)  

---

<a id="architecture"></a>
## ğŸ§  Architecture

<a id="langgraph-flow"></a>
### ğŸ” LangGraph Flow

```mermaid
flowchart TD
    A[START] --> B[agent (LLM w/ tools)]
    B -->|tool call| C[retrieve (ToolNode)]
    B -->|no tool call| Z[END]

    C --> D{grade_documents}
    D -->|relevant| E[generate (RAG answer)]
    D -->|not relevant| F[rewrite (improve query)]
    F --> B

    E --> Z[END]
Key idea: the LLM is â€œtoolâ€‘aware.â€ If it chooses to call a retriever tool, the graph routes into retrieval. Otherwise, it can respond immediately and exit.

ğŸ“š Knowledge Base & Retrieval Tools
On startup, the app downloads and embeds content from four URLs, then builds two FAISS indexes:
ğŸ§© Agents sources

* https://lilianweng.github.io/posts/2023-06-23-agent/
* https://lilianweng.github.io/posts/2024-04-12-diffusion-video/

ğŸ“ RAGAS sources

* https://www.analyticsvidhya.com/blog/2024/05/a-beginners-guide-to-evaluating-rag-pipelines-using-ragas/
* https://docs.ragas.io/en/stable/#why-ragas

Each FAISS index is exposed to the agent as a LangChain retriever tool:

* ğŸ” Agentic_blog: â€œSearch and run information about agentsâ€
* ğŸ” Ragas_blog: â€œSearch and run information about RAGAS (RAG Assessment) frameworkâ€

Retrieval configuration

* âœ‚ï¸ Splitter: RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
* ğŸ¯ Retriever: MMR search with k=5 and lambda_mult=0.5



ğŸš€ Getting Started

âœ… Prerequisites

* 
ğŸ Python 3.10+ recommended

* 
ğŸŒ Internet access (documents are loaded from the web at runtime)

* 
ğŸ”‘ An API key for at least one supported provider:

âœ… OpenAI (implemented)
âœ… Groq (implemented)




ğŸ’¡ Note: OpenRouter/Gemini appear in the UI, but the provided code currently falls back to ChatOpenAI for â€œotherâ€ providers (see Known Limitations).


ğŸ“¦ Installation
Create and activate a virtual environment, then install dependencies:
bashDownloadCopy codepython -m venv .venv
# Windows:
.venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate

pip install -U pip
pip install -r requirements.txt
Example requirements.txt (adjust versions as needed):
txtDownloadCopy codestreamlit
langchain
langchain-openai
langchain-groq
langchain-community
langchain-text-splitters
langchain-huggingface
langgraph
faiss-cpu
sentence-transformers
pydantic
beautifulsoup4
requests

â–¶ï¸ Run the App
bashDownloadCopy codestreamlit run app.py


âš™ï¸ Configuration (UI)
In the Streamlit sidebar:
OptionDescriptionAI ProviderOpenAI / Groq / OpenRouter / GeminiModelProviderâ€‘specific model listAPI KeyEntered securely via password inputMax Response TokensCaps generation lengthShow Agent StepsReveals nodeâ€‘byâ€‘node execution output + JSON payloadsRebuild PipelineClears st.cache_resource, rebuilds:â€¢ embeddingsâ€¢ FAISS indexesâ€¢ retriever tools


ğŸ” How It Works (Stepâ€‘byâ€‘step)

1. 
ğŸ“š Build vector stores (cached)

Tries to load a local HuggingFace embedding model from a hardcoded path.
If not found, falls back to: sentence-transformers/all-MiniLM-L6-v2.
Loads the 4 web pages, splits them into chunks, embeds them, and creates 2 FAISS indexes.
Wraps each retriever as a tool the agent can call.


2. 
â“ User asks a question

Query comes from the input box or suggested question buttons.
The app instantiates the selected LLM (ChatOpenAI or ChatGroq) with:
temperature=0
max_tokens from the sidebar


3. 
ğŸ¤– Agent node (LLM + tools)
The LLM is bound to retriever tools (bind_tools). It either:

âœ… Responds directly (no tool calls) â†’ graph ends, OR
ğŸ”§ Requests retrieval (tool call) â†’ graph routes to retrieve.


4. 
ğŸ§° Retrieve node
Executes the chosen retriever tool and returns documents as ToolMessage outputs.

5. 
âœ… Grade documents
A structuredâ€‘output check returns a strict "yes" / "no" relevance signal.

If any retrieved chunk is graded relevant â†’ proceed to generate.
Otherwise â†’ proceed to rewrite.


6. 
â™»ï¸ Rewrite
The model rewrites the query to better match the knowledge base.
Special handling: If the query mentions â€œRagasâ€ and retrieval failed, it clarifies it means the AI evaluation framework.
Loops back to the agent node to try again.

7. 
âœï¸ Generate
A RAG prompt combines user question + retrieved context.
The model produces a grounded answer (or says â€œI donâ€™t knowâ€).

8. 
ğŸ’¬ UI rendering
Conversation history is shown with st.chat_message.
âœ… Optional: Node execution logs appear in the â€œAgent Reasoning Stepsâ€ expander.




ğŸ› ï¸ Customization

1. ğŸŒ Add / change knowledge sources
Edit the URL lists in build_vectorstore():


* urls_1 â†’ agentâ€‘related sources
* urls_2 â†’ RAGASâ€‘related sources


1. ğŸšï¸ Tune chunking & retrieval (in build_vectorstore())


* chunk_size / chunk_overlap â†’ controls context granularity
* Retriever config:

k: number of chunks retrieved
lambda_mult: MMR diversity vs. similarity balance




1. 
ğŸ§¬ Swap embedding model
Replace model_name= with any HuggingFace embedding model compatible with HuggingFaceEmbeddings.

2. 
ğŸ’¾ Persist FAISS indexes (recommended improvement)
Currently, FAISS is built inâ€‘memory (cached per Streamlit session). Extend the project to save/load FAISS to disk for faster startups.

3. 
ğŸ”Œ Implement OpenRouter / Gemini properly
The UI lists them, but the code uses a generic fallback for â€œotherâ€ providers. Add the correct LangChain integrations + environment variables to fully support them.




ğŸ§¯ Troubleshooting
IssueSolutionâŒ â€œPlease enter your API Keyâ€¦â€Enter a valid provider key and rerun.âš ï¸ Embedding model load fails1. The hardcoded local path may not exist on your machine.2. Ensure sentence-transformers is installed.3. Verify internet access (fallback downloads the model).ğŸŒ Web pages fail to load / empty docsâ€¢ Source sites may block scraping or change HTML.â€¢ Your network may block requests.Fix: Replace URLs or use a different loader.ğŸ§© Nothing appears in final answerâœ… Enable Show Agent Steps to inspect which node executed and what it returned.ğŸ” Rebuild doesnâ€™t change anythingClick Rebuild Pipeline to clear st.cache_resource and rebuild the knowledge base.


âš ï¸ Known Limitations

1. 
ğŸ’¬ No true conversation memory in the graph
The UI shows chat history, but each run sends only the latest query into the graph:
HumanMessage(content=final_query).

2. 
ğŸ§ª OpenRouter/Gemini are not fully implemented
They appear in the UI, but the â€œelseâ€ branch uses ChatOpenAI as a generic fallback.

3. 
ğŸ’¾ No persistent vector database
FAISS is created inâ€‘memory (cached by Streamlit). Restarting the app rebuilds embeddings unless persistence is added.

4. 
ğŸªŸ Hardcoded local embedding path
The Windowsâ€‘specific path likely wonâ€™t exist on other machines (fallback handles this, but consider replacing it).




ğŸ” Security Notes

1. 
ğŸ”‘ API Key Handling
Your key is entered in the sidebar and assigned to environment variables at runtime (os.environ[...]).
Best Practices:

âœ… Never commit secrets to Git!
âœ… Use Streamlit secrets or deploymentâ€‘level environment variables.
âœ… Add .env to .gitignore (see Project Structure).


2. 
ğŸŒ External Web Requests
The app fetches external web pages at runtime. If deploying publicly, consider:

Rate limiting
Caching / persistence
Allowlists for trusted domains





ğŸ—ºï¸ Roadmap Ideas

* ğŸ”Œ Add real multiâ€‘provider support (OpenRouter, Gemini) with proper SDKs
* ğŸ’¾ Persist FAISS indexes; rebuild only when sources change
* ğŸ§  Add conversation memory (feed prior messages into the graph)
* ğŸ§¾ Add citations (source URL + chunk snippet) in the final answer
* ğŸ“ Integrate RAGAS evaluation directly (very onâ€‘theme!)



ğŸ™ Acknowledgements / Sources
This demo knowledge base is built from:
Lilian Weng

* https://lilianweng.github.io/posts/2023-06-23-agent/
* https://lilianweng.github.io/posts/2024-04-12-diffusion-video/

RAGAS references

* https://docs.ragas.io/en/stable/#why-ragas
* https://www.analyticsvidhya.com/blog/2024/05/a-beginners-guide-to-evaluating-rag-pipelines-using-ragas/



ğŸ“„ License
Add your preferred license (e.g., MIT) in a LICENSE file.


ğŸ“ Project Structure
Below is a clean, scalable structure for this project (recommended for organizing the provided code):
agentic-rag-project/
â”‚
â”œâ”€â”€ app.py                    # Main Streamlit application
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ .env                      # Environment variables (gitignored)
â”œâ”€â”€ .gitignore                # Git ignore rules
â”‚
â”œâ”€â”€ utils/                    # Utility functions
â”‚   â”œâ”€â”€ vector_store.py       # FAISS & embedding management
â”‚   â””â”€â”€ agent_graph.py        # LangGraph workflow definition
â”‚
â”œâ”€â”€ config/                   # Configuration files
â”‚   â””â”€â”€ prompts.py            # Prompt templates
â”‚
â””â”€â”€ README.md                 # This documentation

âœ… Suggested Module Responsibilities
utils/vector_store.py

* build_vectorstore()
* Document loading (URLs)
* Splitting + embedding + FAISS creation
* Retriever tool creation

utils/agent_graph.py

* AgentState definition
* initialize_graph(llm_model, tools_list)
* Nodes: agent, retrieve, grade_documents, rewrite, generate

config/prompts.py

* Grading prompt template
* RAG generation prompt template
* Rewrite prompt template


ğŸ“„ .gitignore (minimal)
gitignoreDownloadCopy code.env
.venv/
__pycache__/
*.pyc
.streamlit/
